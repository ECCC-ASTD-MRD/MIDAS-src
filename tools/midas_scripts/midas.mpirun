#!/bin/bash

## Ce script est un wrapper autour de `r.run_in_parallel` pour rassembler
## le code qui doit être exécuté avant de lancer le MPI.

set -ex

__midas_mpirun_pgm__=$1
shift
__midas_mpirun_msg__=$1
shift
__midas_mpirun_ssm__=$1
shift
__midas_mpirun_OMP_STACKSIZE__=$1
shift
## Cette variable doit inclure l'appel a 'r.run_in_parallel' comme par exemple
##     ${TASK_BIN}/r.run_in_parallel -npex 10 -npey 10
__midas_mpirun_call_program__=$@

if [ -x ${SEQ_BIN}/nodelogger ]; then
    __midas_mpirun_nodelogger__=${SEQ_BIN}/nodelogger
else
    __midas_mpirun_nodelogger__=$(which nodelogger || true)
    if [ -z "${__midas_mpirun_nodelogger__}" ]; then
	function nodelogger {
	    echo $*
	}
	__midas_mpirun_nodelogger__=nodelogger
    fi
fi

${__midas_mpirun_nodelogger__} -n $SEQ_NODE -s infox -m  "Launching program ${__midas_mpirun_msg__}"
unset __midas_mpirun_msg__ __midas_mpirun_nodelogger__

export OMP_STACKSIZE=${__midas_mpirun_OMP_STACKSIZE__}
unset __midas_mpirun_OMP_STACKSIZE__

if [ "${EC_ARCH}" = Linux_x86-64 ]; then
    set +x
    for __midas_mpirun_ssmdomain__ in ${__midas_mpirun_ssm__}; do
	echo "loading SSM domain '${__midas_mpirun_ssmdomain__}'"
	. ssmuse-sh -d ${__midas_mpirun_ssmdomain__}
    done
    unset __midas_mpirun_ssmdomain__
    set -x

    echo "loading OMPI_MCA variables"
    export OMPI_MCA_orte_tmpdir_base=/run/shm
    export OMPI_MCA_btl_openib_if_include=mlx5_0
elif [ "${EC_ARCH}" = ubuntu-18.04-skylake-64 ]; then
    set +x
    for __midas_mpirun_ssmdomain__ in ${__midas_mpirun_ssm__}; do
	echo "loading SSM domain '${__midas_mpirun_ssmdomain__}'"
	. ssmuse-sh -d ${__midas_mpirun_ssmdomain__}
    done
    unset __midas_mpirun_ssmdomain__
    set -x
elif [ "${EC_ARCH}" = sles-11-amd64-64/PrgEnv-intel-5.2.82 -o "${EC_ARCH}" = sles-11-broadwell-64-xc40/PrgEnv-intel-5.2.82 ]; then
    ## load 'dot-tools' compiled without any MPI libs on Cray
    ## This will allow us to use 'cclargs' in MPI scripts.  It was
    ## impossible before because 'cclargs' was using MPI and 'aprun'
    ## can't launch two MPI binaries in the same call.
    . ssmuse-sh -x comm/eccc/all/base/master/dot-tools_2.13-gcc_sles-11-amd64-64
    export CCLARGS=$(which cclargs)
elif [ "${EC_ARCH}" = sles-15-skylake-64-xc50/PrgEnv-intel-6.0.5 ]; then
    ## This allows us to save 7% of computation time without changing the results!
    ## export TBB_MALLOC_USE_HUGE_PAGES=1
    module load craype-hugepages16M
    export FOR_DISABLE_KMP_MALLOC=0
else
    echo "midas.mpirun: The plateform 'EC_ARCH=${EC_ARCH}' is not supported (only 'AIX-powerpc7', 'Linux_x86-64', 'ubuntu-18.04-skylake-64', 'sles-15-skylake-64-xc50/PrgEnv-intel-6.0.5' and 'sles-11-amd64-64/PrgEnv-intel-5.2.82' are)"
    exit 1
fi

## find 'npex' and 'npey' in '${__midas_mpirun_call_program__}'
npex=1
npey=1
set -- ${__midas_mpirun_call_program__}
while [ $# -ne 0 ]; do
    case $1 in
        -npex)
            npex=$2
            shift
            ;;
        -npey)
            npey=$2
            shift
            ;;
    esac
    shift
done

SECONDS=0
## create indiviual trace for each rank
for rank in $(seq 0 $((npex*npey-1))); do
    date +%Y%m%d:%H:%M:%S.%N > touch.before.launch.${rank}
done
echo "The files 'touch.before.launch.*' have been created in ${SECONDS} seconds"

cat > midas.mpi_script.sh <<EOF
#!/bin/bash

set -ex

SECONDS=0

status=0
${__midas_mpirun_pgm__} \$@ || status=1

if [ "\${status}" -ne 0 ]; then
    echo "The MPI execution aborted!"
    exit 1
fi

$(which echo) MP_CHILD=\${MP_CHILD} \$($(which date) +%Y%m%d:%H:%M:%S.%N) before erasing touch.before.launch.\${MP_CHILD}
$(which rm)   ${PWD}/touch.before.launch.\${MP_CHILD}
$(which echo) MP_CHILD=\${MP_CHILD} \$($(which date) +%Y%m%d:%H:%M:%S.%N) after erasing touch.before.launch.\${MP_CHILD}

echo SECONDS=\${SECONDS}

EOF
chmod u+x midas.mpi_script.sh


status=0

SECONDS=0
${__midas_mpirun_call_program__} -processorder -tag -nocleanup -verbose -pgm ${PWD}/midas.mpi_script.sh || status=1
echo "The whole MPI execution took ${SECONDS} seconds"

unset __midas_mpirun_call_program__ __midas_mpirun_pgm__

SECONDS=0

if [ "${status}" -ne 0 ]; then
    echo "The whole MPI execution aborted!"
    exit 1
fi

## Check if all individual MPI tiles ran without errors
abortedRanks=$(/bin/ls touch.before.launch.* 2>/dev/null || true)
if [ -n "${abortedRanks}" ]; then
    echo "Some MPI ranks aborted!"
    echo ${abortedRanks}
    echo "The execution verification took ${SECONDS} seconds"
    exit 1
fi

echo "The execution verification took ${SECONDS} seconds"
